2024-02-26 10:41:27,610 - Environment info:
------------------------------------------------------------
sys.platform: win32
Python: 3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:16:33) [MSC v.1929 64 bit (AMD64)]
CUDA available: True
CUDA_HOME: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.3
NVCC: Not Available
GPU 0: NVIDIA GeForce RTX 4080
GCC: <built-in method strip of str object at 0x000001F721CBE1F0>
PyTorch: 2.1.0
PyTorch compiling details: PyTorch built with:
  - C++ Version: 199711
  - MSVC 192930151
  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 2019
  - LAPACK is enabled (usually provided by MKL)
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.8.1  (built against CUDA 12.0)
  - Magma 2.5.4
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.8.1, CXX_COMPILER=C:/cb/pytorch_1000000000000/work/tmp_bin/sccache-cl.exe, CXX_FLAGS=/DWIN32 /D_WINDOWS /GR /EHsc /bigobj /FS -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE /utf-8 /wd4624 /wd4068 /wd4067 /wd4267 /wd4661 /wd4717 /wd4244 /wd4804 /wd4273, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=OFF, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=OFF, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.16.0
OpenCV: 4.8.1
openstl: 1.0.0
------------------------------------------------------------

2024-02-26 10:41:27,611 - 
device: 	cuda	
dist: 	False	
res_dir: 	work_dirs	
ex_name: 	custom_exp	
fp16: 	False	
torchscript: 	False	
seed: 	42	
fps: 	False	
test: 	False	
deterministic: 	False	
batch_size: 	2	
val_batch_size: 	1	
num_workers: 	8	
data_root: 	./data	
dataname: 	custom	
pre_seq_length: 	12	
aft_seq_length: 	12	
total_length: 	24	
use_augment: 	False	
use_prefetcher: 	False	
drop_last: 	False	
method: 	predrnnv2	
config_file: 	None	
model_type: 	gSTA	
drop: 	0.0	
drop_path: 	0.1	
overwrite: 	False	
loss: 	mse	
epoch: 	50	
log_step: 	1	
opt: 	adam	
opt_eps: 	None	
opt_betas: 	None	
momentum: 	0.9	
weight_decay: 	0.0	
clip_grad: 	None	
clip_mode: 	norm	
no_display_method_info: 	False	
sched: 	onecycle	
lr: 	0.001	
lr_k_decay: 	1.0	
warmup_lr: 	1e-06	
min_lr: 	1e-06	
final_div_factor: 	10000.0	
warmup_epoch: 	5	
decay_epoch: 	100	
decay_rate: 	0.1	
filter_bias_and_bn: 	False	
gpus: 	[0]	
metric_for_bestckpt: 	val_loss	
metrics: 	['mae', 'mse']	
in_shape: 	[12, 1, 144, 144]	
num_hidden: 	128,128,128,128	
filter_size: 	5	
stride: 	1	
patch_size: 	2	
layer_norm: 	0	
decouple_beta: 	0.1	
reverse_scheduled_sampling: 	1	
r_sampling_step_1: 	25000	
r_sampling_step_2: 	50000	
r_exp_alpha: 	5000	
scheduled_sampling: 	1	
sampling_stop_iter: 	50000	
sampling_start_value: 	1.0	
sampling_changing_rate: 	2e-05	
model_num: 	3	
2024-02-26 10:41:27,611 - Model info:
PredRNNv2_Model(
  (criterion): MSELoss()
  (cell_list): ModuleList(
    (0): SpatioTemporalLSTMCellv2(
      (conv_x): Sequential(
        (0): Conv2d(4, 896, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
      )
      (conv_h): Sequential(
        (0): Conv2d(128, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
      )
      (conv_m): Sequential(
        (0): Conv2d(128, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
      )
      (conv_o): Sequential(
        (0): Conv2d(256, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
      )
      (conv_last): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (1-3): 3 x SpatioTemporalLSTMCellv2(
      (conv_x): Sequential(
        (0): Conv2d(128, 896, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
      )
      (conv_h): Sequential(
        (0): Conv2d(128, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
      )
      (conv_m): Sequential(
        (0): Conv2d(128, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
      )
      (conv_o): Sequential(
        (0): Conv2d(256, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
      )
      (conv_last): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
  )
  (conv_last): Conv2d(128, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (adapter): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
)
| module                   | #parameters or shape   | #flops     |
|:-------------------------|:-----------------------|:-----------|
| model                    | 23.585M                | 2.826T     |
|  cell_list               |  23.568M               |  2.81T     |
|   cell_list.0            |   3.809M               |   0.454T   |
|    cell_list.0.conv_x.0  |    89.6K               |    10.683G |
|    cell_list.0.conv_h.0  |    1.638M              |    0.195T  |
|    cell_list.0.conv_m.0  |    1.229M              |    0.147T  |
|    cell_list.0.conv_o.0  |    0.819M              |    97.675G |
|    cell_list.0.conv_last |    32.768K             |    3.907G  |
|   cell_list.1            |   6.586M               |   0.785T   |
|    cell_list.1.conv_x.0  |    2.867M              |    0.342T  |
|    cell_list.1.conv_h.0  |    1.638M              |    0.195T  |
|    cell_list.1.conv_m.0  |    1.229M              |    0.147T  |
|    cell_list.1.conv_o.0  |    0.819M              |    97.675G |
|    cell_list.1.conv_last |    32.768K             |    3.907G  |
|   cell_list.2            |   6.586M               |   0.785T   |
|    cell_list.2.conv_x.0  |    2.867M              |    0.342T  |
|    cell_list.2.conv_h.0  |    1.638M              |    0.195T  |
|    cell_list.2.conv_m.0  |    1.229M              |    0.147T  |
|    cell_list.2.conv_o.0  |    0.819M              |    97.675G |
|    cell_list.2.conv_last |    32.768K             |    3.907G  |
|   cell_list.3            |   6.586M               |   0.785T   |
|    cell_list.3.conv_x.0  |    2.867M              |    0.342T  |
|    cell_list.3.conv_h.0  |    1.638M              |    0.195T  |
|    cell_list.3.conv_m.0  |    1.229M              |    0.147T  |
|    cell_list.3.conv_o.0  |    0.819M              |    97.675G |
|    cell_list.3.conv_last |    32.768K             |    3.907G  |
|  conv_last               |  0.512K                |  61.047M   |
|   conv_last.weight       |   (4, 128, 1, 1)       |            |
|  adapter                 |  16.384K               |  15.628G   |
|   adapter.weight         |   (128, 128, 1, 1)     |            |
--------------------------------------------------------------------------------

2024-02-26 11:22:26,565 - Epoch 1: Lr: 0.0000400 | Train Loss: 0.0152627 | Vali Loss: 0.0062056
2024-02-26 11:42:57,611 - Epoch 2: Lr: 0.0000400 | Train Loss: 0.0048517 | Vali Loss: 0.0065214
2024-02-26 12:03:23,115 - Epoch 3: Lr: 0.0000400 | Train Loss: 0.0040246 | Vali Loss: 0.0072701
2024-02-26 12:23:49,568 - Epoch 4: Lr: 0.0000400 | Train Loss: 0.0037453 | Vali Loss: 0.0085624
2024-02-26 12:44:16,506 - Epoch 5: Lr: 0.0000400 | Train Loss: 0.0034069 | Vali Loss: 0.0221668
2024-02-26 13:04:42,068 - Epoch 6: Lr: 0.0000400 | Train Loss: 0.0032679 | Vali Loss: 0.0057362
2024-02-26 13:25:09,581 - Epoch 7: Lr: 0.0000400 | Train Loss: 0.0034422 | Vali Loss: 0.0109589
2024-02-26 13:45:35,221 - Epoch 8: Lr: 0.0000400 | Train Loss: 0.0031101 | Vali Loss: 0.0057123
2024-02-26 14:06:02,444 - Epoch 9: Lr: 0.0000400 | Train Loss: 0.0029539 | Vali Loss: 0.0056371
2024-02-26 14:26:29,121 - Epoch 10: Lr: 0.0000400 | Train Loss: 0.0029585 | Vali Loss: 0.0061917
2024-02-26 14:46:54,885 - Epoch 11: Lr: 0.0000400 | Train Loss: 0.0028424 | Vali Loss: 0.0055024
2024-02-26 15:07:22,006 - Epoch 12: Lr: 0.0000400 | Train Loss: 0.0027998 | Vali Loss: 0.0073343
2024-02-26 15:27:47,734 - Epoch 13: Lr: 0.0000400 | Train Loss: 0.0026578 | Vali Loss: 0.0051555
2024-02-26 15:48:14,392 - Epoch 14: Lr: 0.0000400 | Train Loss: 0.0026082 | Vali Loss: 0.0048957
2024-02-26 16:08:41,589 - Epoch 15: Lr: 0.0000400 | Train Loss: 0.0025211 | Vali Loss: 0.0048946
2024-02-26 16:29:08,601 - Epoch 16: Lr: 0.0000400 | Train Loss: 0.0024723 | Vali Loss: 0.0055522
2024-02-26 16:49:35,111 - Epoch 17: Lr: 0.0000400 | Train Loss: 0.0025081 | Vali Loss: 0.0050415
2024-02-26 17:10:02,618 - Epoch 18: Lr: 0.0000400 | Train Loss: 0.0022907 | Vali Loss: 0.0046019
2024-02-26 17:30:30,007 - Epoch 19: Lr: 0.0000400 | Train Loss: 0.0022066 | Vali Loss: 0.0077331
2024-02-26 17:50:55,528 - Epoch 20: Lr: 0.0000400 | Train Loss: 0.0021132 | Vali Loss: 0.0048606
2024-02-26 18:11:21,238 - Epoch 21: Lr: 0.0000400 | Train Loss: 0.0021033 | Vali Loss: 0.0057604
2024-02-26 18:31:46,833 - Epoch 22: Lr: 0.0000400 | Train Loss: 0.0020345 | Vali Loss: 0.0043213
2024-02-26 18:52:12,820 - Epoch 23: Lr: 0.0000400 | Train Loss: 0.0019812 | Vali Loss: 0.0058036
2024-02-26 19:12:38,662 - Epoch 24: Lr: 0.0000400 | Train Loss: 0.0019424 | Vali Loss: 0.0042276
2024-02-26 19:33:04,729 - Epoch 25: Lr: 0.0000400 | Train Loss: 0.0019050 | Vali Loss: 0.0043131
2024-02-26 19:53:31,321 - Epoch 26: Lr: 0.0000400 | Train Loss: 0.0019105 | Vali Loss: 0.0046810
2024-02-26 20:13:58,634 - Epoch 27: Lr: 0.0000400 | Train Loss: 0.0018139 | Vali Loss: 0.0044678
2024-02-26 20:34:25,294 - Epoch 28: Lr: 0.0000400 | Train Loss: 0.0017475 | Vali Loss: 0.0053334
2024-02-26 20:54:51,890 - Epoch 29: Lr: 0.0000400 | Train Loss: 0.0017185 | Vali Loss: 0.0044099
2024-02-26 21:15:18,735 - Epoch 30: Lr: 0.0000400 | Train Loss: 0.0016506 | Vali Loss: 0.0038729
2024-02-26 21:35:45,475 - Epoch 31: Lr: 0.0000400 | Train Loss: 0.0017407 | Vali Loss: 0.0039636
2024-02-26 21:56:12,287 - Epoch 32: Lr: 0.0000400 | Train Loss: 0.0014603 | Vali Loss: 0.0041682
2024-02-26 22:16:39,247 - Epoch 33: Lr: 0.0000400 | Train Loss: 0.0015213 | Vali Loss: 0.0044212
2024-02-26 22:37:06,026 - Epoch 34: Lr: 0.0000400 | Train Loss: 0.0014495 | Vali Loss: 0.0046510
2024-02-26 22:57:32,637 - Epoch 35: Lr: 0.0000400 | Train Loss: 0.0014637 | Vali Loss: 0.0041890
2024-02-26 23:17:54,679 - Epoch 36: Lr: 0.0000400 | Train Loss: 0.0014287 | Vali Loss: 0.0056694
2024-02-26 23:38:17,609 - Epoch 37: Lr: 0.0000400 | Train Loss: 0.0014150 | Vali Loss: 0.0040401
2024-02-27 00:03:05,953 - Epoch 38: Lr: 0.0000400 | Train Loss: 0.0014329 | Vali Loss: 0.0038551
2024-02-27 00:23:28,917 - Epoch 39: Lr: 0.0000400 | Train Loss: 0.0014438 | Vali Loss: 0.0039247
2024-02-27 00:43:48,826 - Epoch 40: Lr: 0.0000400 | Train Loss: 0.0014412 | Vali Loss: 0.0040338
2024-02-27 01:04:08,362 - Epoch 41: Lr: 0.0000400 | Train Loss: 0.0014389 | Vali Loss: 0.0036340
2024-02-27 01:24:28,357 - Epoch 42: Lr: 0.0000400 | Train Loss: 0.0014906 | Vali Loss: 0.0037422
2024-02-27 01:44:47,288 - Epoch 43: Lr: 0.0000400 | Train Loss: 0.0015155 | Vali Loss: 0.0051720
2024-02-27 02:05:07,169 - Epoch 44: Lr: 0.0000400 | Train Loss: 0.0014983 | Vali Loss: 0.0036835
2024-02-27 02:25:27,018 - Epoch 45: Lr: 0.0000400 | Train Loss: 0.0015581 | Vali Loss: 0.0036554
2024-02-27 02:45:46,946 - Epoch 46: Lr: 0.0000400 | Train Loss: 0.0015396 | Vali Loss: 0.0039274
2024-02-27 03:06:05,765 - Epoch 47: Lr: 0.0000400 | Train Loss: 0.0015883 | Vali Loss: 0.0035066
2024-02-27 03:26:25,407 - Epoch 48: Lr: 0.0000400 | Train Loss: 0.0016262 | Vali Loss: 0.0039399
2024-02-27 03:46:45,075 - Epoch 49: Lr: 0.0000400 | Train Loss: 0.0016007 | Vali Loss: 0.0035229
2024-02-27 03:47:56,709 - mse:75.06234741210938, mae:608.1808471679688
