2024-03-15 18:07:59,174 - Environment info:
------------------------------------------------------------
sys.platform: win32
Python: 3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:16:33) [MSC v.1929 64 bit (AMD64)]
CUDA available: True
CUDA_HOME: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.3
NVCC: Not Available
GPU 0: NVIDIA GeForce RTX 4080
GCC: <built-in method strip of str object at 0x000001C5F21DA790>
PyTorch: 2.1.0
PyTorch compiling details: PyTorch built with:
  - C++ Version: 199711
  - MSVC 192930151
  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 2019
  - LAPACK is enabled (usually provided by MKL)
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.8.1  (built against CUDA 12.0)
  - Magma 2.5.4
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.8.1, CXX_COMPILER=C:/cb/pytorch_1000000000000/work/tmp_bin/sccache-cl.exe, CXX_FLAGS=/DWIN32 /D_WINDOWS /GR /EHsc /bigobj /FS -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE /utf-8 /wd4624 /wd4068 /wd4067 /wd4267 /wd4661 /wd4717 /wd4244 /wd4804 /wd4273, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=OFF, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=OFF, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.16.0
OpenCV: 4.8.1
openstl: 1.0.0
------------------------------------------------------------

2024-03-15 18:07:59,177 - 
device: 	cuda	
dist: 	False	
res_dir: 	work_dirs	
ex_name: 	custom_exp	
fp16: 	False	
torchscript: 	False	
seed: 	42	
fps: 	False	
test: 	False	
deterministic: 	False	
batch_size: 	4	
val_batch_size: 	1	
num_workers: 	8	
data_root: 	./data	
dataname: 	custom	
pre_seq_length: 	12	
aft_seq_length: 	12	
total_length: 	24	
use_augment: 	False	
use_prefetcher: 	False	
drop_last: 	False	
method: 	convlstm	
config_file: 	None	
model_type: 	gSTA	
drop: 	0.0	
drop_path: 	0.1	
overwrite: 	False	
loss: 	mse	
epoch: 	50	
log_step: 	1	
opt: 	adam	
opt_eps: 	None	
opt_betas: 	None	
momentum: 	0.9	
weight_decay: 	0.0	
clip_grad: 	None	
clip_mode: 	norm	
no_display_method_info: 	False	
sched: 	onecycle	
lr: 	0.001	
lr_k_decay: 	1.0	
warmup_lr: 	1e-06	
min_lr: 	1e-06	
final_div_factor: 	10000.0	
warmup_epoch: 	5	
decay_epoch: 	100	
decay_rate: 	0.1	
filter_bias_and_bn: 	False	
gpus: 	[0]	
metric_for_bestckpt: 	val_loss	
metrics: 	['mae', 'mse']	
in_shape: 	[12, 1, 144, 144]	
reverse_scheduled_sampling: 	0	
r_sampling_step_1: 	25000	
r_sampling_step_2: 	50000	
r_exp_alpha: 	5000	
scheduled_sampling: 	1	
sampling_stop_iter: 	50000	
sampling_start_value: 	1.0	
sampling_changing_rate: 	2e-05	
num_hidden: 	32,32,32,32	
filter_size: 	5	
stride: 	1	
patch_size: 	2	
layer_norm: 	0	
model_num: 	4	
2024-03-15 18:07:59,177 - Model info:
ConvLSTM_Model(
  (MSE_criterion): MSELoss()
  (cell_list): ModuleList(
    (0): ConvLSTMCell(
      (conv_x): Sequential(
        (0): Conv2d(4, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
      )
      (conv_h): Sequential(
        (0): Conv2d(32, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
      )
      (conv_o): Sequential(
        (0): Conv2d(64, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
      )
      (conv_last): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (1-3): 3 x ConvLSTMCell(
      (conv_x): Sequential(
        (0): Conv2d(32, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
      )
      (conv_h): Sequential(
        (0): Conv2d(32, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
      )
      (conv_o): Sequential(
        (0): Conv2d(64, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
      )
      (conv_last): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
  )
  (conv_last): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)
)
| module                   | #parameters or shape   | #flops     |
|:-------------------------|:-----------------------|:-----------|
| model                    | 0.943M                 | 87.007G    |
|  cell_list               |  0.943M                |  86.992G   |
|   cell_list.0            |   0.168M               |   13.736G  |
|    cell_list.0.conv_x.0  |    12.8K               |    1.526G  |
|    cell_list.0.conv_h.0  |    0.102M              |    12.209G |
|    cell_list.0.conv_o.0  |    51.2K               |            |
|    cell_list.0.conv_last |    2.048K              |            |
|   cell_list.1            |   0.258M               |   24.419G  |
|    cell_list.1.conv_x.0  |    0.102M              |    12.209G |
|    cell_list.1.conv_h.0  |    0.102M              |    12.209G |
|    cell_list.1.conv_o.0  |    51.2K               |            |
|    cell_list.1.conv_last |    2.048K              |            |
|   cell_list.2            |   0.258M               |   24.419G  |
|    cell_list.2.conv_x.0  |    0.102M              |    12.209G |
|    cell_list.2.conv_h.0  |    0.102M              |    12.209G |
|    cell_list.2.conv_o.0  |    51.2K               |            |
|    cell_list.2.conv_last |    2.048K              |            |
|   cell_list.3            |   0.258M               |   24.419G  |
|    cell_list.3.conv_x.0  |    0.102M              |    12.209G |
|    cell_list.3.conv_h.0  |    0.102M              |    12.209G |
|    cell_list.3.conv_o.0  |    51.2K               |            |
|    cell_list.3.conv_last |    2.048K              |            |
|  conv_last               |  0.128K                |  15.262M   |
|   conv_last.weight       |   (4, 32, 1, 1)        |            |
--------------------------------------------------------------------------------

2024-03-15 18:10:05,436 - Epoch 1: Lr: 0.0000400 | Train Loss: 0.0059865 | Vali Loss: 0.0104955
2024-03-15 18:11:08,429 - Epoch 2: Lr: 0.0000400 | Train Loss: 0.0025800 | Vali Loss: 0.0097098
2024-03-15 18:12:12,111 - Epoch 3: Lr: 0.0000400 | Train Loss: 0.0022960 | Vali Loss: 0.0138422
2024-03-15 18:13:14,775 - Epoch 4: Lr: 0.0000400 | Train Loss: 0.0020870 | Vali Loss: 0.0092092
2024-03-15 18:14:17,498 - Epoch 5: Lr: 0.0000400 | Train Loss: 0.0018939 | Vali Loss: 0.0375872
2024-03-15 18:15:24,148 - Epoch 6: Lr: 0.0000400 | Train Loss: 0.0017712 | Vali Loss: 0.0061042
2024-03-15 18:16:29,041 - Epoch 7: Lr: 0.0000400 | Train Loss: 0.0017102 | Vali Loss: 0.0062238
2024-03-15 18:17:31,578 - Epoch 8: Lr: 0.0000400 | Train Loss: 0.0017437 | Vali Loss: 0.0056492
2024-03-15 18:18:34,090 - Epoch 9: Lr: 0.0000400 | Train Loss: 0.0016441 | Vali Loss: 0.0071333
2024-03-15 18:19:36,405 - Epoch 10: Lr: 0.0000400 | Train Loss: 0.0016342 | Vali Loss: 0.0071495
2024-03-15 18:20:38,751 - Epoch 11: Lr: 0.0000400 | Train Loss: 0.0016038 | Vali Loss: 0.0066838
2024-03-15 18:21:41,415 - Epoch 12: Lr: 0.0000400 | Train Loss: 0.0015929 | Vali Loss: 0.0128063
2024-03-15 18:22:43,904 - Epoch 13: Lr: 0.0000400 | Train Loss: 0.0015861 | Vali Loss: 0.0072156
2024-03-15 18:23:46,355 - Epoch 14: Lr: 0.0000400 | Train Loss: 0.0015575 | Vali Loss: 0.0112017
2024-03-15 18:24:48,749 - Epoch 15: Lr: 0.0000400 | Train Loss: 0.0015524 | Vali Loss: 0.0071362
2024-03-15 18:25:51,292 - Epoch 16: Lr: 0.0000400 | Train Loss: 0.0015749 | Vali Loss: 0.0053976
2024-03-15 18:26:54,111 - Epoch 17: Lr: 0.0000400 | Train Loss: 0.0015347 | Vali Loss: 0.0065108
2024-03-15 18:27:56,992 - Epoch 18: Lr: 0.0000400 | Train Loss: 0.0015357 | Vali Loss: 0.0061526
2024-03-15 18:28:59,828 - Epoch 19: Lr: 0.0000400 | Train Loss: 0.0015186 | Vali Loss: 0.0412189
2024-03-15 18:30:02,920 - Epoch 20: Lr: 0.0000400 | Train Loss: 0.0015139 | Vali Loss: 0.0058886
2024-03-15 18:31:06,151 - Epoch 21: Lr: 0.0000400 | Train Loss: 0.0015220 | Vali Loss: 0.0056035
2024-03-15 18:32:09,169 - Epoch 22: Lr: 0.0000400 | Train Loss: 0.0014890 | Vali Loss: 0.0057607
2024-03-15 18:33:11,757 - Epoch 23: Lr: 0.0000400 | Train Loss: 0.0014951 | Vali Loss: 0.0076840
2024-03-15 18:34:14,500 - Epoch 24: Lr: 0.0000400 | Train Loss: 0.0014780 | Vali Loss: 0.0053807
2024-03-15 18:35:17,098 - Epoch 25: Lr: 0.0000400 | Train Loss: 0.0014789 | Vali Loss: 0.0055651
2024-03-15 18:36:19,679 - Epoch 26: Lr: 0.0000400 | Train Loss: 0.0014592 | Vali Loss: 0.0133517
2024-03-15 18:37:22,301 - Epoch 27: Lr: 0.0000400 | Train Loss: 0.0014714 | Vali Loss: 0.0053359
2024-03-15 18:38:24,846 - Epoch 28: Lr: 0.0000400 | Train Loss: 0.0014578 | Vali Loss: 0.0057979
2024-03-15 18:39:27,249 - Epoch 29: Lr: 0.0000400 | Train Loss: 0.0014179 | Vali Loss: 0.0050842
2024-03-15 18:40:29,807 - Epoch 30: Lr: 0.0000401 | Train Loss: 0.0014401 | Vali Loss: 0.0051602
2024-03-15 18:41:32,440 - Epoch 31: Lr: 0.0000401 | Train Loss: 0.0014241 | Vali Loss: 0.0063901
2024-03-15 18:42:35,240 - Epoch 32: Lr: 0.0000401 | Train Loss: 0.0014026 | Vali Loss: 0.0047162
2024-03-15 18:43:38,193 - Epoch 33: Lr: 0.0000401 | Train Loss: 0.0013942 | Vali Loss: 0.0050689
2024-03-15 18:44:40,994 - Epoch 34: Lr: 0.0000401 | Train Loss: 0.0013818 | Vali Loss: 0.0060286
2024-03-15 18:45:43,938 - Epoch 35: Lr: 0.0000401 | Train Loss: 0.0014272 | Vali Loss: 0.0060874
2024-03-15 18:46:46,987 - Epoch 36: Lr: 0.0000401 | Train Loss: 0.0013607 | Vali Loss: 0.0050056
2024-03-15 18:47:49,905 - Epoch 37: Lr: 0.0000401 | Train Loss: 0.0013739 | Vali Loss: 0.0050155
2024-03-15 18:48:53,001 - Epoch 38: Lr: 0.0000401 | Train Loss: 0.0013309 | Vali Loss: 0.0055198
2024-03-15 18:49:55,920 - Epoch 39: Lr: 0.0000401 | Train Loss: 0.0013404 | Vali Loss: 0.0046523
2024-03-15 18:50:58,900 - Epoch 40: Lr: 0.0000401 | Train Loss: 0.0013323 | Vali Loss: 0.0056049
2024-03-15 18:52:01,999 - Epoch 41: Lr: 0.0000401 | Train Loss: 0.0013426 | Vali Loss: 0.0049174
2024-03-15 18:53:04,931 - Epoch 42: Lr: 0.0000401 | Train Loss: 0.0013210 | Vali Loss: 0.0120727
2024-03-15 18:54:08,093 - Epoch 43: Lr: 0.0000401 | Train Loss: 0.0013208 | Vali Loss: 0.0055687
2024-03-15 18:55:10,574 - Epoch 44: Lr: 0.0000401 | Train Loss: 0.0013191 | Vali Loss: 0.0088860
2024-03-15 18:56:13,238 - Epoch 45: Lr: 0.0000401 | Train Loss: 0.0013137 | Vali Loss: 0.0045438
2024-03-15 18:57:15,825 - Epoch 46: Lr: 0.0000401 | Train Loss: 0.0013078 | Vali Loss: 0.0072770
2024-03-15 18:58:18,043 - Epoch 47: Lr: 0.0000401 | Train Loss: 0.0013446 | Vali Loss: 0.0046725
2024-03-15 18:59:20,325 - Epoch 48: Lr: 0.0000401 | Train Loss: 0.0013194 | Vali Loss: 0.0048299
2024-03-15 19:00:22,598 - Epoch 49: Lr: 0.0000401 | Train Loss: 0.0013172 | Vali Loss: 0.0048588
2024-03-15 19:00:27,855 - mse:106.97071838378906, mae:680.6796875
